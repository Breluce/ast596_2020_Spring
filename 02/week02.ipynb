{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'height': 1080, 'scroll': True, 'width': 1920}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1920,\n",
    "        'height': 1080,\n",
    "        'scroll': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 02, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "### Gautham Narayan \n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "* We've done an overview of \"descriptive\" statistics (as opposed to inferential statistics)\n",
    "* Random variables (discrete and continuous), sample estimators, covariance\n",
    "    * Bayes' rule and how it follows from the axioms of probability\n",
    "    * conditional probability, marginalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Distributions (univariate and multivariate), moments, common distributions, PDFs, CDFs\n",
    "* Random variables as samples from a distribution \n",
    "* Why you should look at the samples, not just moments/summary statistics\n",
    "* Comparing samples to a normal distribution (the QQ plot), identifying skewness, outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The central limit theorem\n",
    "* Basic visualizations of multivariate data\n",
    "* Dealing with basic astronomical data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Remember the goal is to get to <i>P(H|D).</i> </center>\n",
    "\n",
    "### <center> What questions do you have? </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Method of Moments and Hypothesis Testing \n",
    "\n",
    "\n",
    "### The Method of Moments (Chebyshev, 1887)\n",
    "We know how to estimate moments if we have an underlying description of the population - the PDF\n",
    "\n",
    "### <center> $\\mu_{n} = \\int_{-\\infty}^{\\infty} (x-c)^{n} \\cdot p(x) \\cdot dx $ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Generally, we do not have the PDF but have some random samples that are drawn from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have been equating sample moments with population moments already\n",
    "\n",
    "* the KDE plot with multivariate data\n",
    "* estimating parameters from a multivariate sample by eye and constructing a multivariate Gaussian\n",
    "* your homework..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This works when the data is simple enough to be described by some simple model. You also know when this isn't the case:\n",
    "\n",
    "<img src=\"./figures/DataDino-600x455.gif\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But if it is the case...\n",
    "\n",
    "* Assume a form for the empirical PDF $f(x; \\theta)$\n",
    "* Equate the sample moments with the empirical moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pas\\Miniconda3\\envs\\fds\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['cm']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "import scipy.stats as st\n",
    "from astropy.visualization import hist as ahist\n",
    "import statsmodels.api as sm\n",
    "\n",
    "samp_size = 10\n",
    "\n",
    "# simulate something like the magnitudes you might measure on a detector\n",
    "samp = -2.5*np.log10(st.poisson.rvs(2000, size=samp_size)) + 25\n",
    "\n",
    "# true moments are straightforward - remember change of variables for the standard deviation\n",
    "mu_true = -2.5*np.log10(2000) + 25\n",
    "sig_true = np.log10(e)*2.5*np.sqrt(mu_true)/mu_true\n",
    "\n",
    "# sample moments\n",
    "mu_samp = samp.mean()\n",
    "sig_samp = samp.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-565da037bdce>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-565da037bdce>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    ahist(samp, bins='freedman', density=True, ax=ax1,      label=fr'True $\\mu=${mu_true:.3f}, $\\sigma$={sig_true:.3f}')\u001b[0m\n\u001b[1;37m                                                                                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fig = figure(figsize=(8, 4))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ahist(samp, bins='freedman', density=True, ax=ax1,\\\n",
    "      label=fr'True $\\mu=${mu_true:.3f}, $\\sigma$={sig_true:.3f}')\n",
    "xmin, xmax = ax1.get_xlim()\n",
    "xval = np.arange(xmin, xmax+0.01, 0.001)\n",
    "estpdf  = st.norm.pdf(xval, loc=mu_samp, scale=sig_samp)\n",
    "ax1.plot(xval, estpdf, color='C1', lw=2, label=fr'Est $\\mu=${mu_samp:.3f}, $\\sigma$={sig_samp:.3f}')\n",
    "ax1.legend(frameon=False)\n",
    "sm.qqplot(samp, line='r', ax=ax2)\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is closely related to the **Null Hypothesis Rejection Test** (NHRT)\n",
    "\n",
    "* You have data drawn from some unknown population\n",
    "* Assume a model for that population\n",
    "    * This allows you to make a prediction for the data you have\n",
    "    * \"Under the null hypothesis\" = if the model is a good description of the data\n",
    "        * then the data should be highly probable\n",
    "    * Define a test such that some observation has a very low probability of happening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Given your data, if the low probability happened, then you can reject your model\n",
    "* If it hasn't happened, your model still holds\n",
    "    * but that doesn't tell you it's a good description of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center> <img src='./figures/nhrt.png'> </center>\n",
    "(credit: Federica Bianco, U. Delaware)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So when you hear \"$x$ is a 2-$\\sigma$ detection!\"\n",
    "\n",
    "* 2-$\\sigma$: confidence level\n",
    "* 0.05: p-value\n",
    "* 95%: threshold\n",
    "\n",
    "\n",
    "Unstated here is that there is an assumption of some model, typically a Gaussian.\n",
    "\n",
    "**Nothing** here states that the model is a good description of the data at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider this from Kang et al. \"Early-type Host Galaxies of Type Ia Supernovae. II. Evidence for Luminosity Evolution in Supernova Cosmology\", 2020, ApJ, 889, Issue 1, id. 8\n",
    "\n",
    "[ADS Link](https://ui.adsabs.harvard.edu/abs/2020ApJ...889....8K/abstract)\n",
    "or \n",
    "[ArXiv](https://arxiv.org/abs/1912.04903)\n",
    "\n",
    "  <center> <img src =\"./figures/kang_sne_evol.png\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Staistical sins\n",
    "\n",
    "- hypotheses suggested by non-representative data\n",
    "    - you almost always have to deal with this in real data\n",
    "- looking for patterns in your data is legitimate\n",
    "    - applying a hypothesis test to the same data from which a pattern emerges is not\n",
    "        - randomized out-of-sample tests/cross-validation\n",
    "- p is not the probability that the hypothesis is false! (remember what you are given here is the hypothesis)\n",
    "- p-hacking: only reporting the hypothesis with a significant p-value but not reporting the ones without\n",
    "\n",
    "#### Read Cohen 1994 \"The Earth is Round (p < .05)\" included in the directory for all the issues with hypothesis testing\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If your sample is indeed drawn from a random distribution:\n",
    "\n",
    "### <center> $ \\left| x - \\mu \\right| > 3\\sigma$ implies $X$ is more extreme than the distribution 0.27% of the time </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In class exercise 1\n",
    "### NRHT when the model isn't normal\n",
    "\n",
    "The Universe has many processes that throw outliers into your nice curated data\n",
    "(I hope you are enjoying your homework)\n",
    "\n",
    "If you model a sample as normal, but the underlying population isn't then you are liable to flag something as significant when it isn't. \n",
    "\n",
    "See what difference it makes when you go from a Gaussian to a Student's t-distribution with 8 degrees of freedom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# your code goes here:\n",
    "\n",
    "\n",
    "# You can evaluate what P(X > x) is with the survival function i.e. (1 -  CDF(X))\n",
    "\n",
    "fig = figure(figsize=(10, 5))\n",
    "x = np.arange(-5, 5.01, 0.01)\n",
    "pdf = st.norm.pdf(x)\n",
    "sf = st.norm.sf(x)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax1.plot(x, pdf)\n",
    "ax2.plot(x, sf)\n",
    "\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('p(x)')\n",
    "\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('SF(x)')\n",
    "\n",
    "# DO THIS FOR A T-DISTRIBUTION WITH Nu=8\n",
    "# \n",
    "fig = figure(figsize=(10, 5))\n",
    "t = np.arange(-5, 5.01, 0.01)\n",
    "pdf_t = st.t.pdf(t, df=8)\n",
    "sf_t = st.t.sf(t, df=8)\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax1.plot(t, pdf_t)\n",
    "ax2.plot(t, sf_t)\n",
    "\n",
    "ax1.set_xlabel('t')\n",
    "ax1.set_ylabel('p(t)')\n",
    "\n",
    "ax2.set_xlabel('t')\n",
    "ax2.set_ylabel('SF(t)')\n",
    "\n",
    "# another thing you can try is the interval function to ask what interval encloses % of the total probability\n",
    "# this is better for assymetric distributions\n",
    "print(sf_t[800], sf[800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The issue with the method of moments and classical hypothesis testing is they are not very **robust** to outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "samp_size = 200\n",
    "outlier_fraction1 = 0.1\n",
    "outlier_fraction2 = 0.05\n",
    "\n",
    "# simulate something like the magnitudes you might measure on a detector\n",
    "samp1 = -2.5*np.log10(st.poisson.rvs(2000, size=samp_size)) + 25\n",
    "samp2 = -2.5*np.log10(st.poisson.rvs(1850, size=max(int(outlier_fraction1*samp_size), 1))) + 25\n",
    "samp3 = -2.5*np.log10(st.poisson.rvs(3700, size=max(int(outlier_fraction2*samp_size), 1))) + 25\n",
    "samp = np.concatenate((samp1, samp2, samp3), axis=-1)\n",
    "np.random.shuffle(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# sample moments\n",
    "mu_samp = samp.mean()\n",
    "sig_samp = samp.std()\n",
    "\n",
    "fig = figure(figsize=(8, 4))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ahist(samp, bins='freedman', density=True, ax=ax1)\n",
    "xmin, xmax = ax1.get_xlim()\n",
    "xval = np.arange(xmin, xmax+0.01, 0.001)\n",
    "estpdf  = st.norm.pdf(xval, loc=mu_samp, scale=sig_samp)\n",
    "ax1.plot(xval, estpdf, color='r', lw=2, label=fr'Est $\\mu=${mu_samp:.3f}, $\\sigma$={sig_samp:.3f}')\n",
    "ax1.legend(frameon=False)\n",
    "sm.qqplot(samp, line='r', ax=ax2, marker='.')\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In class exercise 2\n",
    "### Robust Statistics - Alternative 1: Eliminating Outliers (i.e. clean your data)\n",
    "\n",
    "In your homework, you have encountered one way to cleanup your data, using the QQ-plot.\n",
    "* using a line defined through some quartiles\n",
    "* using $Y = \\mu + \\sigma\\cdot x$ \n",
    "\n",
    "Both of these methods are fundamentally assuming the data is normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "astropy.stats has a sigma_clip method that also assumes the data is normally distributed, but you can control around where. In particular you can use the median.\n",
    "\n",
    "Try eliminating the outliers with it, and check if the method of moments estimator gives you something reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# your code goes here - don't overwrite samp\n",
    "from astropy.stats import sigma_clip\n",
    "samp_clipped = sigma_clip(samp, sigma=2, masked=False)\n",
    "fig = figure(figsize=(8, 4))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ahist(samp, bins='freedman', density=True, ax=ax1)\n",
    "ahist(samp_clipped, bins='freedman', density=True, ax=ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\sigma$-clipping is probably the most widely used outlier rejection method in the astronomy literature.\n",
    "\n",
    "However it is *ad hoc* - $\\sigma$ itself is changing each clipping iteration, but you cannot know how unless you know the underlying PDF that the sample was drawn from, including the outliers.\n",
    "\n",
    "It's also painfully slow with large quantities of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Alternative 2: L-estimators\n",
    "\n",
    "It's useful to have some summary statistics that describe distributions, which are robust to outliers, and can be computed fast.\n",
    "\n",
    "We assumed a form for the empirical PDF $f(x; \\theta)$, and used some moments as estimators. But there are other quantities that you can get from the empirical PDF that are more robust. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The quantile estimators we looked at in week 1 already have this behavior.\n",
    "\n",
    "### <center> $ p = \\int_{-\\infty}^{x_p} f(x) dx $ </center>\n",
    "\n",
    "### and the median: <center> $ \\frac{1}{2} = \\int_{-\\infty}^{x_{\\frac{1}{2}} } f(x) dx $ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can also define measures of the variance of the data. This was one - you've used it on your homework implicitly.\n",
    "\n",
    "### <center> $ \\text{IQR} = x_{\\frac{3}{4}} - x_{\\frac{1}{4}} $</center>\n",
    "\n",
    "For a normal distribution, $\\sigma = \\text{IQR}/2\\sqrt{2}\\cdot\\text{erf}^{-1}(\\frac{1}{2}) \\;\\; (\\approx1.349)$. This is another common L-estimator called the **Median Absolute Deviation**\n",
    "\n",
    "### <center> $ \\text{MAD}  = \\left| x_i - x_{\\frac{1}{2}} \\right| $ </center>\n",
    "\n",
    "For a normal distribution, $\\sigma = 1.4826\\cdot\\text{MAD}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# if we repeat the exercise with robust estimators\n",
    "median = np.median(samp)\n",
    "sig_robust = 1.4826*st.median_absolute_deviation(samp)\n",
    "sig_robust2 = st.iqr(samp)/1.349\n",
    "\n",
    "fig = figure(figsize=(8, 4))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ahist(samp, bins='freedman', density=True, ax=ax1)\n",
    "xmin, xmax = ax1.get_xlim()\n",
    "xval = np.arange(xmin, xmax+0.01, 0.001)\n",
    "\n",
    "# this time, instead of applying the method of moments using the sample moments, we're using the L-estimators\n",
    "estpdf  = st.norm.pdf(xval, loc=median, scale=sig_robust)\n",
    "estpdf2  = st.norm.pdf(xval, loc=median, scale=sig_robust2)\n",
    "\n",
    "ax1.plot(xval, estpdf, color='C2', lw=2, label=fr'Est $\\mu=${median:.3f}, $\\sigma$={sig_robust:.3f}')\n",
    "ax1.plot(xval, estpdf2, color='C4', lw=2, label=fr'Est $\\mu=${median:.3f}, $\\sigma$={sig_robust2:.3f}')\n",
    "ax1.legend(frameon=False)\n",
    "\n",
    "# and the QQ plot\n",
    "sm.qqplot(samp, line='r', ax=ax2, marker='.')\n",
    "nmin, nmax = ax2.get_xlim()\n",
    "nvals = np.arange(nmin, nmax+0.01, 0.01)\n",
    "y = median + sig_robust*nvals\n",
    "y2 = median + sig_robust2*nvals\n",
    "ax2.plot(nvals, y, '--', color='C2', lw=2, label='Using MAD')\n",
    "ax2.plot(nvals, y2, '-.', color='C4', lw=2, label='Using IQR')\n",
    "ax2.legend(frameon=False)\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "L-estimators work well when:\n",
    "   * the data has outliers\n",
    "   * the data has no (or at least very small) uncertainties associated with it\n",
    "       * notice that we computed them from the sample, but made no mention/use of sample uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Alternative 3: M-estimators\n",
    "\n",
    "* As before, assume a form for the empirical PDF $f(x; \\theta)$\n",
    "* Define a **loss function** $\\rho(u)$ for some quantity $u$ e.g. $x - \\mu$\n",
    "* Minimize this loss over the sample\n",
    "    * $\\sum_{i=1}^{N} \\rho(u_i) $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### OK, so what the heck is a loss function now?\n",
    "\n",
    "If $(x_1, x_2..., x_N)$ is a set of i.i.d random variables from some distribution p(x) which we don't know, but want to estimate. \n",
    "\n",
    "What we're trying to do is to build an estimator for moments out of the sample.\n",
    "\n",
    "If we define\n",
    "\n",
    "## <center> $ \\rho(x, \\theta=\\mu) = \\frac{(x - \\mu)^2}{2} $ </center>\n",
    "\n",
    "How do you go about minimizing this with respect to $\\theta$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#argmin - index of minimum (to find x values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For $u = x-\\mu$, there's several potential cost functions:\n",
    "\n",
    "### <center> $\\rho(u) = u^2$ </center>\n",
    "    \n",
    "is an old friend to all of you. It's the **sum of squared residuals** or what we sometimes call the $L_2$ norm.\n",
    "\n",
    "Of the loss functions, this is the nicest behaved - it's **convex** and **differentiable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Differentiability implies continuity. Continuity does not guarantee differentiability. \n",
    "\n",
    "Most M-estimators aren't nicely differentiable, but often have other properties - such as resistance to outliers.\n",
    "\n",
    "You've already seen another loss function:\n",
    "\n",
    "### <center> $\\rho(u) = \\left|u \\right|$ </center>\n",
    "\n",
    "This is the sum of absolute residuals and is related directly to the MAD. You might see it called the $L_1$ norm, particularly in machine learning literature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Other loss functions you'll see\n",
    "\n",
    "<center> <img src=\"./figures/loss_functions.jpg\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### M-estimators and The Likelihood Function\n",
    "\n",
    "If we know the distribution from which our data were drawn (or make a hypothesis about it), then we can compute the **probability** of our data being generated.\n",
    "\n",
    "For example, for the Gaussian distribution probablity of getting a specific value of $x$ is given by:\n",
    "\n",
    "$$p(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Likelihood Function\n",
    "\n",
    "If we want to know the total probability of our *entire* data set (as opposed to one measurement) then we must compute the *product* of all the individual probabilities:\n",
    "$$L \\equiv p(\\{x_i\\}|H(\\theta)) = \\prod_{i=1}^n p(x_i|H(\\theta)),$$\n",
    "where $H$ refers to the *hypothesis* and $\\theta$ refers collectively to the $k$ parameters of the model, which can generally be multi-dimensional. In words, this is ***the probability of the data given the model parameters***. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "(Note we're assuming the individual measurements are independent of each other.)\n",
    "\n",
    "If we consider $L$ as a function of the model parameters, we refer to it as the ***likelihood of the model parameters, given the observed data***. \n",
    "\n",
    "Note that while the components of $L$ may be normalized pdfs, their product is not.  Also the product can be very small, so we often take the log of $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can write this out as:\n",
    "$$L = \\prod_{i=1}^n \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This can simplify to\n",
    "$$L = \\left( \\prod_{i=1}^n \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right) \\exp\\left( -\\frac{1}{2} \\sum \\left[\\frac{-(x_i-\\mu)}{\\sigma} \\right]^2 \\right),$$\n",
    "\n",
    "where we have written the product of the exponentials as the exponential of the sum of the arguments, which will make things easier to deal with later.\n",
    "\n",
    "i.e. we have done this: $$\\prod_{i=1}^n A_i \\exp(-B_i) = (A_iA_{i+1}\\ldots A_n) \\exp[-(B_i+B_{i+1}+\\ldots+B_n)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**NOTE**\n",
    "\n",
    "The Likelihood function is the joint **probability density** of all measurements. For each measurment we have a probablity density. These probability densities have **units**. They are the reciprical of the units of the measurement (as the integrated probability does not have units but the interval over which we are integrating does)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The argument of the exponential is just\n",
    "\n",
    "$$\\exp \\left(-\\frac{\\chi^2}{2}\\right).$$\n",
    "\n",
    "where, for our Gaussian distribution\n",
    "$$\\chi^2 = \\sum_{i=1}^n \\left ( \\frac{x_i-\\mu}{\\sigma}\\right)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So, maximizing the likelihood is the same as minimizing $\\chi^2$.\n",
    "\n",
    "\n",
    "i.e. maximum likelihood estimation - and $\\chi^2$ minimization are just a special case of M-estimators. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Cramer Rao bound\n",
    "- Likelihood ratio\n",
    "- AIC/BIC"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:fds]",
   "language": "python",
   "name": "conda-env-fds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

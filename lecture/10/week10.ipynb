{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "import george\n",
    "from george import kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 10, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## Hierarchical Bayes and Modeling Populations \n",
    "\n",
    "### Gautham Narayan \n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "Last week, we saw how:\n",
    "\n",
    "1. A Gaussian process is specified by a choice for mean and covariance kernel\n",
    "2. For a fixed choice of mean and covariance kernel, but no data, you have a Gaussian process prior - an infinite number of functions can be drawn from this prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a kernel \n",
    "k = 3*kernels.ExpSquaredKernel(metric=2.)\n",
    "\n",
    "# Define a Gaussian Process - you need a mean and a kernel \n",
    "gp = george.GP(mean=0, kernel=k)\n",
    "\n",
    "# Evaluate the kernel on the abscissa values/exogenous variable/x/t\n",
    "t = np.arange(0, 100)\n",
    "gp.compute(t)\n",
    "\n",
    "# draw some random samples from the Gaussian process prior\n",
    "samples = gp.sample(t, size=5)\n",
    "\n",
    "# and plot them\n",
    "plt.plot(t, samples.T)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "3. This framework is really flexible, and you can use it to make predictions for physical processes even when you can't write down an explicit model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define some function of the exogenous variable\n",
    "def func(t):\n",
    "    ''' \n",
    "    some completely whacky function that we might never imagine to write down as a model\n",
    "    '''\n",
    "    np.random.seed(42)\n",
    "    y = - 3*((t/20)-30)**2 - 0.001*(t-20)**3 + 20*np.random.randn(len(t))\n",
    "    ind = ((t/18)%2)<=1\n",
    "    y[ind] += 90*np.sin(np.pi/4*t[ind] )\n",
    "    return y\n",
    "\n",
    "# evaluate the function at abscissa values\n",
    "y = func(t)\n",
    "\n",
    "\n",
    "# and plot them\n",
    "plt.scatter(t, y, marker='o', label='some wonky function')\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "4. You have to condition the model on the data (i.e. evaluate the posterior) if you want to make predictions (either interpolating or extrapolating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define a GP with the same kernel as before, but a mean specified by the mean of the data\n",
    "gp = george.GP(mean=np.mean(y), kernel=k)\n",
    "\n",
    "\n",
    "# again, evaluate the kernel at the abscissa values - we'll only use every other value\n",
    "gp.compute(t[::2])\n",
    "\n",
    "# setup a new array of times for prediction\n",
    "t_new = np.arange(-30, 131, 0.1)\n",
    "\n",
    "# now *CONDITION* the GP on the observed values y, and predict on the new values t_new\n",
    "ypred1 = gp.predict(y[::2], t_new, return_cov=False)\n",
    "\n",
    "# and plot the original data + the GP prediction with the constant mean\n",
    "plt.scatter(t, y, color='C0', marker='o', label='some wonky function')\n",
    "plt.scatter(t[::2], y[::2], color='C3', marker='o', label='The data we used to condition the GP')\n",
    "\n",
    "plt.plot(t_new, ypred1, 'C1', ls='--', label='conditioned, unoptimized GP\\nw/ constant mean', lw=3)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "5. If you want your predictions to be reasonable, you should think about how to specify the mean function well.\n",
    "    \n",
    "Advice: Use the mean function to specify the things that can you model reasonably, and use the Gaussian process to describe the bit you cannot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# maybe we can't write down a model, but we can say looking a the data that there is some polynomial trend\n",
    "coeffs = np.polyfit(t, y, 3)\n",
    "\n",
    "# plot the data with the fitted polynomial\n",
    "plt.scatter(t, y, color='C0', marker='o', label='some wonky function')\n",
    "plt.plot(t, np.polyval(coeffs, t), color='C2', ls='--', label='Some 3rd order polynomial\\nto describe the mean', lw=3)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as op\n",
    "from george.modeling import Model\n",
    "\n",
    "# define a mean model defined by the polynomial we just found\n",
    "class PolynomialModel(Model):\n",
    "    parameter_names = ()\n",
    "    global coeffs\n",
    "    def get_value(self, t):\n",
    "        t = t.flatten()\n",
    "        return (np.polyval(coeffs, t))\n",
    "\n",
    "    \n",
    "# now use the polynomial as the mean model - but we're not fitting those values again - just the GP parameters\n",
    "gp = george.GP(mean=PolynomialModel(), kernel=k)\n",
    "\n",
    "\n",
    "# Define the objective function (negative log-likelihood in this case).\n",
    "def nll(p):\n",
    "    gp.set_parameter_vector(p)\n",
    "    ll = gp.log_likelihood(y[::2], quiet=True)\n",
    "    return -ll if np.isfinite(ll) else 1e25\n",
    "\n",
    "# And the gradient of the objective function.\n",
    "def grad_nll(p):\n",
    "    gp.set_parameter_vector(p)\n",
    "    return -gp.grad_log_likelihood(y[::2], quiet=True)\n",
    "\n",
    "# You need to compute the GP once before starting the optimization.\n",
    "gp.compute(t[::2])\n",
    "\n",
    "# Run the optimization routine.\n",
    "p0 = gp.get_parameter_vector()\n",
    "results = op.minimize(nll, p0, jac=grad_nll, method=\"L-BFGS-B\")\n",
    "\n",
    "# Update the kernel and print the final log-likelihood.\n",
    "gp.set_parameter_vector(results.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# now make a new prediction with our new mean model\n",
    "ypred2, cov = gp.predict(y[::2], t_new, return_cov=True)\n",
    "std = np.sqrt(np.diag(cov))\n",
    "\n",
    "# plot the data\n",
    "plt.scatter(t, y, color='C0', marker='o', label='some wonky function')\n",
    "plt.scatter(t[::2], y[::2], color='C3', marker='o', label='The data we used to condition the GP')\n",
    "\n",
    "# and plot the truth\n",
    "plt.plot(t_new, func(t_new), 'C4', ls='-', lw=0.5, label='Truth', alpha=0.5)\n",
    "\n",
    "# plot our initial prediction with a constant mean\n",
    "plt.plot(t_new, ypred1, 'C1', ls='--', label='conditioned, unoptimized GP\\nw/ constant mean', lw=3)\n",
    "\n",
    "# plot our new prediction with the polynomial mean \n",
    "plt.plot(t_new, ypred2, 'C2', ls='-.', label='conditioned, optimized GP\\nw/ poly3 mean', lw=3)\n",
    "\n",
    "# plot the scatter about the posterior predictive mean from the diagonal of the covariance matrix \n",
    "plt.fill_between(t_new, ypred2-std, ypred2+std, color='lightgrey')\n",
    "\n",
    "\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hint for HW7:\n",
    "\n",
    "- The same skeleton we're using here will work for HW7. \n",
    "\n",
    "- There you have a more complex kernel + a model that has parameters of it's own \n",
    "    - (here we pretend we \"know\" our polynomial coefficients as far as the mean model goes\n",
    "\n",
    "- You also have WAY MORE observations - you don't need to condition your GP on ALL THE DATA!!\n",
    "    - this is better for looking for consistency\n",
    "    \n",
    "- Start with fitting the mean model, then add the GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "\n",
    "\n",
    "The \"ExpSquared\" (Squared Exponential) kernel we used above is:\n",
    "\n",
    "\\begin{equation}\n",
    "k_{1}\\left(t, t^{\\prime}\\right)=\\theta_{1}^{2} \\exp \\left(-\\frac{\\left(t-t^{\\prime}\\right)^{2}}{2 \\theta_{2}^{2}}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "If, for a specific set of values for $\\theta_{1},\\theta_{2}$ in your chosen kernel, your Gaussian process prior corresponds to a family of functions, and you are not sure of the values of $\\theta_{1},\\theta_{2}$ itself, then you can imagine $\\theta_{1},\\theta_{2}$ to be parameters themselves.\n",
    "\n",
    "### These are parameters of a prior distribution - we call these **hyperparameters** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How we pick priors:\n",
    "\n",
    "1. If we have a previous/independent measurement/inference of the parameter etc., use it with its error bars as $p(\\theta)$. (You've done this)\n",
    "\n",
    "2. Choose wide, uninformative distributions for all the parameters we don’t know well. (You've done this)\n",
    "\n",
    "3. **Use distributions in nature from previous observations of similar objects i.e. the population.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Option 3: Use distributions in nature from previous observations of similar objects.\n",
    "\n",
    "\n",
    "Histograms of population properties, when normalized, can be interpreted as probability distributions for individual parameters:\n",
    "\n",
    "\n",
    "## $$\n",
    "\\mathbf{p}(\\theta)=\\mathbf{n}(\\theta | \\mathbf{\\alpha}) / \\int \\mathbf{n}(\\theta | \\mathbf{\\alpha}) \\mathbf{d} \\mathbf{\\theta}=\\mathbf{p}(\\theta | \\mathbf{\\alpha})\n",
    "$$\n",
    "\n",
    "\n",
    "where $n(\\theta|\\alpha)$ is the function with parameters $\\alpha$ that was fit to the histogram\n",
    "\n",
    "$\\theta$ = parameter\n",
    "\n",
    "$\\alpha$ = hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Going Hierarchical:\n",
    "\n",
    "Bayes theorem:\n",
    "\n",
    "## $$ p(\\theta|x) = p(x|\\theta)\\cdot p(\\theta) $$\n",
    "\n",
    "Abstracting:\n",
    "## $$\\Bigg\\downarrow$$\n",
    "\n",
    "## $$ p(\\theta|x) = p(x|\\theta)\\cdot p(\\theta | \\alpha) $$\n",
    "\n",
    "i.e. the population helps make inference on individual ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is this hierarchical Bayesian framework good for?\n",
    "\n",
    "### If you have multiple sets of measurements, each of which are related, we may want to learn something not just about each individual measurement, *but also the population*\n",
    "\n",
    "In quantitative psychology for example you test multiple subjects on the same task. We then want to estimate a computational/mathematical model that describes the behavior on the task by a set of parameters. \n",
    "\n",
    "- We could thus fit a model to each subject individually, assuming they share no similarities; \n",
    "- or, pool all the data and estimate one model assuming all subjects are identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"tacos.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Hierarchical modeling** allows the best of both worlds by modeling subjects’ similarities but also allowing estimiation of individual parameters.\n",
    "\n",
    "In elections, everyone is looking at the same ballot nationally, and we care about the overall population, but maybe we also care about how states/counties differ from each other.\n",
    "\n",
    "The same is true of countries during a global pandemic..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "E.g. your midterm - you have multiple galaxies, each with a potentially indpendent period-luminosity relation (indeed many of you treated it as such) because they have different ages/star formation history/whatever.\n",
    "\n",
    "Maybe you are interested in the P-L relation for just 1 galaxy, but maybe you also care about the population of all of them so you can derive the Hubble constant.\n",
    "\n",
    "I didnd't have you do that for the slopes, but I did for the intercepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Going Hierarchical: Inferring properties of a population\n",
    "\n",
    "... but what if we want to use the individuals to infer things (the $\\alpha$’s) about the population?\n",
    "\n",
    "i.e., **$p(\\theta|\\alpha)$ contains interesting physics and getting values for $\\alpha$ given the data (individuals) can help us understand the population**\n",
    "\n",
    "## $$ p(\\theta|x) = p(x|\\theta)\\cdot p(\\theta | \\alpha) $$\n",
    "\n",
    "## $$\\Bigg\\downarrow$$\n",
    "\n",
    "## $$ p(\\alpha, \\theta|x) = p(x|\\theta)\\cdot p(\\theta | \\alpha) \\cdot p(\\alpha)$$\n",
    "\n",
    "For your midterm, where we could have fit individual slopes $\\theta_i$ we did the extreme thing of $p(\\theta_i | \\alpha) = \\delta(\\theta_i - \\alpha)$ and fit just one global slope $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"hierarchical1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"hierarchical2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Build up complexity by layering conditional probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## And if you really don't care the parameters for individual measurements, just marginalize over them\n",
    "\n",
    "## $$\n",
    "p(\\alpha |x) \\propto\\left[\\int p(x | \\theta, \\alpha) p(\\theta | \\alpha) d\\theta\\right] p(\\alpha)=p(x | \\alpha) p(\\alpha)\n",
    "$$\n",
    "\n",
    "(This is what happens with your Gaussian Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Hierarchical modeling is a statistically rigorous way  to make scientific inferences about a population (or specific object) based on many individuals (or observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"hbm1.png\">\n",
    "\n",
    "courtesy: Tom Loredo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"hbm2.png\">\n",
    "\n",
    "courtesy: Tom Loredo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WHY does this work??\n",
    "\n",
    "- All 18 players are humans playing baseball—they are members of a population, not arbitrary, unrelated binomial random number generators!\n",
    "\n",
    "- In the absence of data about player $i$, we may use the performance of the other players to guide a guess about that player’s performance—they provide indirect evidence about player $i$\n",
    "\n",
    "- But information that is relevant in the absence of data for $i$ remains relevant when we additionally obtain that data; shrinkage estimators account for this\n",
    "\n",
    "- There is “mustering and borrowing of strength” (Tukey) across the population\n",
    "\n",
    "- Hierarchical Bayesian modeling is the most flexible framework for generalizing this lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## There's many examples of situations where we care about populations:\n",
    "\n",
    "<img src=\"kepler_mass_radius.png\">\n",
    "\n",
    "[Kepler Planets from Lissauer, Dawson and Tremaine, 2014](https://ui.adsabs.harvard.edu/abs/2014Natur.513..336L/abstract)\n",
    "\n",
    "\n",
    "### Once we discover an object, we look for more to characterize their properties and understand their origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Or maybe we use many noisy observations of a single object to understand it's physics.\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"SN1987A_R.jpg\"></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"SN1987A_B.jpg\"></th>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "[Larrson et al., 2019](https://ui.adsabs.harvard.edu/abs/2019ApJ...886..147L/abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"hierarchical3.png\">\n",
    "Courtesy: Angie Wolfgang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"hierarchical4.png\">\n",
    "Courtesy: Angie Wolfgang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"hierarchical5.png\">\n",
    "Courtesy: Angie Wolfgang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"hierarchical6.png\">\n",
    "Courtesy: Angie Wolfgang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"hierarchical7.png\">\n",
    "\n",
    "Courtesy: Angie Wolfgang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### TODO:\n",
    "- More on conditional independence and PGMs for HBMs\n",
    "- Cepheid PL as a HBM\n",
    "- Selection effects in HBM"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:fds] *",
   "language": "python",
   "name": "conda-env-fds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1920,\n",
    "        'height': 1080,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 07, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## Posterior Predictive Checks, and connecting the Bayesian and Frequentist Worlds\n",
    "#### (not on your midterm)\n",
    "\n",
    "### Gautham Narayan \n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> How is the Midterm going? </center>\n",
    "\n",
    "\n",
    "## <center> What questions do you have? </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"review.png\" width=100%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "## <center> What do you feel least comfortable with? (Remember the stats dept. offers semester long courses on any of these topics!) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap:\n",
    "\n",
    "We've talked about:\n",
    "* Rejection sampling\n",
    "* **Metropolis-Hastings**\n",
    "    * Random walks are robust but inefficient - suppress random walk behavior to improve efficiency at the cost of complexity (interpretability) and applicability \n",
    "* The behavior or MCMC (burnin, autocorrelation, impact of starting position, if things are stationary...)\n",
    "* How to tell if samples were useful (mixing chains, G-R statistic, number of effective samples, **looking at your data**) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap:\n",
    "\n",
    "More useful MCMC tools:\n",
    "* **Affine-invariant MC** (emcee) - works great as long as posterior is \"nice\" after affine transformation\n",
    "    * counter-examples: Rosenbrock function, eggbox \n",
    "* **Parallel-tempering** (now, ptemcee) - adds chains at multiple temperatures (we care about T=1) \n",
    "    * connection to simulated annealing\n",
    "    * computationally more intensive, even with a low number of dimensions\n",
    "* **Gibbs Sampling** (special choice of prior, but if applicable to your problem, acceptance fraction = 1.)\n",
    "    * connection to probability integral transform \n",
    "    * not very general, but most common way to deal with high dimensional spaces \n",
    "* Useful if overlooked - **you don't have to update all of the parameters of your model the same way**\n",
    "    * i.e. you can look up something like **Metropolis within Gibbs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap:    \n",
    "    \n",
    "* All of these are specializations of MH (different starting positions, different number of chains, different proposals distributions) \n",
    "    * **None require evaluation of derivatives** - which is the general situation you'll be in with astrophysics\n",
    "    * There are packages that will do automatic differentiation for you (e.g. `pymc3` which uses `theano`, `tensorflow`), but these can be \"fragile\" with real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Big Picture: Frequentist Statistics\n",
    "\n",
    "* **Frequentists** make statements about the data (or statistics or estimators= functions of the data), conditional on the parameter: \n",
    "\n",
    "# $$p(D|\\theta) \\mathrm{\\;or\\;} p(f(D) |\\theta)$$\n",
    "\n",
    "* The goal is to get a “point estimate” or confidence intervals with good properties/coverage under “long-run” repeated experiments in the magic wonderland of Asymptopia.\n",
    "    * Confidence intervals - arguments are based on datasets that could have happened but didn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Big Picture: Bayesian Statistics\n",
    "\n",
    "* Bayesians make statements about the probability of parameters conditional on the dataset $D$ that you actually observed\n",
    "\n",
    "# $$p(\\theta|D)$$\n",
    "\n",
    "* This requires an interpretation of probability as a quantifying degree of belief in a hypothesis. This exists without any data even - i.e. **the prior**\n",
    "    * Credible regions - arguments are based on variables you wish you observed but didn't (nuisance parameters/latent variables) \n",
    "    * This is not an equivalent shortcoming to the frequentist approach - it actually matches how reality works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Big Picture: MCMC\n",
    "\n",
    "* The Bayesian answer is the full posterior density, quantifying the \"state of knowledge\" after seeing the data\n",
    "    * The likelihood is not a probability density in the parameters.\n",
    "        * But multiply by a prior (even flat) and the posterior is a probability that obeys clear rules:\n",
    "            * Conditional/Marginal probability\n",
    "* **Numerical estimates (such as samples using Monte Carlo methods) are attempts to (imperfectly) summarize the posterior**\n",
    "    * These techniques give us ways to deal with high-dimensional spaces e.g. many latent variables\n",
    "        * Convert messy integrals to simple sums over samples\n",
    "            * **All those frequentist statistics/estimators are still useful given MCMC samples!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "    \n",
    "## Advice: MCMC\n",
    "\n",
    "* MCMC is a terrible optimizer. If you just want the \"best-fit\", some local/global optimizer is often quicker.\n",
    "    * These are often useful for reasonable starting guesses\n",
    "    * Bayesians will often need to introduce latent variables/nuisance parameters/things you don't observe but wish you did \n",
    "        * YOU SHOULD THINK ABOUT THIS FOR Q3 ON THE MIDTERM!!! \n",
    "            * Coming up with the likelihood is not the same as writing down the model\n",
    "    * These parameters make the problem very high dimensional, even if you don't care about them\n",
    "    * The point of it is to sample the full posterior distribution so you have reasonable **credible regions**\n",
    "        * You are scientists and this is what you actually want\n",
    "            * **You write down the model and likelihood, and the Bayesian framework tells you what distribution of parameters is feasible given your data and your prior belief**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# For Fun: MCMC sampler visulations for different functions, without an annoying soundtrack:\n",
    "\n",
    "# <center>[http://chi-feng.github.io/mcmc-demo/](http://chi-feng.github.io/mcmc-demo/)</center>\n",
    "\n",
    "(There is only so many times you can listen to the Harlem shake - Hungarian dances for sorting algorithms are more fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Posterior-predictive checks\n",
    "\n",
    "* Nothing about the Bayesian framework we've discussed tells us if our model is right\n",
    "    * MCMC can give us very precise, but very wrong inferences, if the model itself is inadequate\n",
    "     \n",
    "     \n",
    "Let's look at the posterior again\n",
    "## $$ P(\\theta|D) \\mathrm{\\; is \\; really \\;} P(\\theta|D, H) $$ \n",
    "i.e. assuming the hypothesis $H$ is itself correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Posterior-predictive checks\n",
    "\n",
    "Frequentists have a way to express the question we're asking\n",
    "\n",
    "## $$P(D|H)$$\n",
    "i.e. \"how likely is the data given the hypothesis\", which is similar to but not exactly the same as  \"how likely is the data given the model parameters of this hypothesis\"\n",
    "\n",
    "The two are related though!\n",
    "\n",
    "## $$P(D|H) = \\int_{\\theta} P(D|\\theta) \\cdot P(\\theta|H)$$\n",
    "\n",
    "This is the **predictive distribution** - the distribution of imaginary datasets if the hypothesis/model is true.\n",
    "\n",
    "* i.e. if you have some observations $y_D$, and you can infer a model and then ask what we would expect to see in hypothetical replications of the same experiment.\n",
    "\n",
    "* if the model is right, you expect to see something similar to what you did the first time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Posterior-predictive checks\n",
    "\n",
    "* THIS IS A FREQUENTIST IDEA!\n",
    "* The idea is to generate data from the model using parameters from draws from the posterior.\n",
    "\n",
    "## Big picture\n",
    "\n",
    "* Bayes theory is needed to *estimate* parameters, conditional on observations and a model we are considering\n",
    "* Frequentist theory is needed to *critique* a model conditioned on the data we observe, by exploring if the model actually is *likely* to generate data like our observations in the first place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In Class Exercise:\n",
    "\n",
    "We'll use our data from HW2 to try posterior predictive checks on a simple linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.optimize\n",
    "from astroML.datasets import fetch_hogg2010test\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# AND THIS\n",
    "\n",
    "# Get data: this includes outliers\n",
    "data = fetch_hogg2010test()\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "dy = data['sigma_y']\n",
    "\n",
    "# convert the data to numpy arrays\n",
    "x  = np.array(x)\n",
    "y  = np.array(y)\n",
    "dy = np.array(dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Next, we're going to use `pymc3` to fit this model, but we're going to be naive and use a Gaussian likelihood, despite knowing there are outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # write down expressions for the priors on the slope and intercept of the line \n",
    "    \n",
    "    # https://docs.pymc.io/api/distributions/continuous.html\n",
    "    # they are all in the form of \n",
    "    # Distribution('variable name', parameters of distribution)\n",
    "    # e.g. Normal('blah', mu=0, sigma=1)\n",
    "\n",
    "    # write down your model\n",
    "\n",
    "    \n",
    "    # and write down your likelihood function\n",
    "    # pymc3 accepts keywords, observed for the data\n",
    "\n",
    "    \n",
    "    # and then to sample!\n",
    "    samples_ols = pm.sample(5000, cores=2) # draw 5000 posterior samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Next make a traceplot of your samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# If your traceplot looks ok, get some summary statistics from your trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Now lets see how if our data looks anything like data generated from the model suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "ppc = pm.sample_posterior_predictive(samples_ols, samples=500, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Plot the posterior predictive samples and the data, and 200 draws from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finally, change the likelihood to a Student-T likelihood, and add a prior on $\\nu$, the number of degrees of freedom of the distribution. and compare the posterior predictive samples from this with your earlier OLS samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [PyMC3 implementation of the Outlier model we used with `emcee`](https://docs.pymc.io/notebooks/GLM-robust-with-outlier-detection.html)\n",
    "\n",
    "(Feel free to use on your midterm if you prefer)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:fds] *",
   "language": "python",
   "name": "conda-env-fds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
